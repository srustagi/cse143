1. What are the most frequent 5 unigrams, bigrams, trigrams, 4-grams and 5-grams? And what are their frequencies?
	Positive:
		Unigrams: [('food', 124), ("'s", 88), ('great', 83), ('place', 61), ('restaurant', 55)]
		Bigrams: [(('great', 'food'), 13), (('food', 'excellent'), 12), (('great', 'place'), 9), (('food', 'service'), 8), (('highly', 'recommend'), 8)]
		Trigrams: [(('food', 'wonderful', 'service'), 4), (('millbrae', 'pancake', 'house'), 4), (('recommend', 'place', 'anyone'), 3), (('food', 'even', 'better'), 3), (('would', 'highly', 'recommend'), 3)]
		4-grams: [(('recommend', 'place', 'anyone', 'wants'), 2), (('great', 'food', 'great', 'prices'), 2), (('great', 'food', 'even', 'better'), 2), (('la', 'rosa', 'negra', 'favorite'), 2), (('butternut', 'squash', 'ravioli', 'browned'), 2)]
		5-grams: [(('butternut', 'squash', 'ravioli', 'browned', 'butter'), 2), (('squash', 'ravioli', 'browned', 'butter', 'sage'), 2), (('excellent', 'restaurant', 'food', 'wonderful', 'service'), 1), (('restaurant', 'food', 'wonderful', 'service', 'friendly'), 1), (('food', 'wonderful', 'service', 'friendly', 'attentive'), 1)]
	Negative:
		Unigrams: [(('food',), 149), (('restaurant',), 96), (('us',), 95), (('service',), 89), (("n't",), 88)]
		Bigrams: [(('dining', 'experience'), 12), (('go', 'back'), 10), (('prime', 'rib'), 8), (('number', 'one'), 8), (('coral', 'grill'), 8)]
		Trigrams: [(('never', 'go', 'back'), 4), (('go', 'somewhere', 'else'), 3), (('recommend', 'restaurant', 'anyone'), 3), (('dining', 'experience', 'food'), 2), (('asked', 'talk', 'manager'), 2)]
		4-grams: [(('1', 'chinese', 'bbq', 'restaurant'), 2), (('would', 'recommend', 'restaurant', 'anyone'), 2), (('one', 'worst', 'experiences', 'ever'), 2), (('place', 'nice', 'care', 'bbq'), 1), (('nice', 'care', 'bbq', 'service'), 1)]
		5-grams: [(('place', 'nice', 'care', 'bbq', 'service'), 1), (('nice', 'care', 'bbq', 'service', "n't"), 1), (('care', 'bbq', 'service', "n't", 'forget'), 1), (('bbq', 'service', "n't", 'forget', 'bring'), 1), (('service', "n't", 'forget', 'bring', 'beer'), 1)]

2. What are the collocations that were found for each category?
	Positive: chez capo, highly recommend, pancake house, san francisco, mashed potatoes, wine list, millbrae pancake, rosa negra, several times, worth trip, big city, food excellent, sure try, head chef, something everyone, ala carte, eastern market, outdoor patio, ravioli browned, great food
	Negative: prime rib, coral grill, dining experience, fried rice, number one, crab legs, 227 bistro, taco bell, tourist trap, local boys, needless say, looked like, speak manager, health department, sunset restaurant, wait staff, medium rare, pattio area, food cold, come back

3. Consider the normalized version of the first sentence of the training data. Given the frequency distributions you created during steps 2 and 3, calculate by hand the probability of that sentence, using a bigram model. Show your work.
	bigram model:
		p(first_sentence) = p(w1) * p(w2 | w1)
		p(first_sentence) = p(excellent) * p(restaurant | excellent)
		p(first_sentence) = 38/6553 * 2/38
		p(first_sentence) = 0.000305203723 = 0.03%


4. Consider again the first sentence of the training data, but without stopwords removed. What is the probability of this sentence using a trigram model. You do not need to calculate the number. Just write out the equation with the probabilities you would need to calculate it. What order of Markov Assumption is this model using? What would be order of the Markov assumption for a 4-gram model?
	trigram model:
		p(first_sentence) = p(w1) * p(w2 | w1) * p(w3 | w1, w2)
		p(first_sentence) = p(an) * p(excellent | an) * p(restaurant | an, excellent)

	4-gram:
		it's a 3rd order markov assumption

	
5. Calculate by hand P(the ∪ wine ∪ list) within the positive domain. Show your work.
	P(the ∪ wine ∪ list) = P(the) + P(wine) + P(list) - P(the ^ wine) - P(the ^ list) - P(wine ^ list) + P(the ^ wine ^ list)
	P(the ∪ wine ∪ list) = 0/6553 + 17/6553 + 12/6553 - (0/6553 * 17/6553) - (0/6553 * 12/6553) - (12/6553 * 17/6553) + (0/6553 * 17/6553 * 12/6553)
	P(the ∪ wine ∪ list) = 0.00442070337 = 0.44%
	
	
6. What happens if you encounter a word that is not in your frequency tables when calculating the probability of an unseen sentence (a sentence that is not in your training data)?
	You can apply a smoothing technique, which will either infer data for the unknown or discount it in some way, so that a probability can be acquired.
	
7. A higher order n-gram (4-gram, 5-gram and so on) model is a better language model than a bi-gram or tri-gram model. Would you say this statement is correct? Please state your reasons.
	I would say this is incorrect, because in the examples from our assignment, the 5- and 4- grams lose the sentiment of the reviews. Consequently, a higher order n-gram is not necessarily better than a lower order n-gram.
